üß† LLM-Powered Intelligent Query‚ÄìRetrieval System
================================================

üìò Project Plan & AI Implementation Instructions

This document outlines the complete design, functionality, architecture, and implementation plan for building a high-quality LLM-powered query-response system for large, unstructured documents. This guide will be read by an AI agent to make code changes, so each section includes detailed and clear instructions.

üìå OBJECTIVE
-----------
Design and implement a system that:
- Accepts documents (PDF, DOCX, emails)
- Parses user queries in natural language
- Retrieves relevant clauses based on semantic similarity
- Uses an LLM to generate structured, explainable answers
- Outputs answers in a specified JSON format

Use cases include: insurance policy reading, legal clause extraction, HR and compliance document Q&A.

‚öôÔ∏è INPUT FORMAT
--------------
Your backend should accept a POST request to `/api/v1/hackrx/run` with the following JSON structure:

{
  "documents": "DOCUMENT_BLOB_URL",
  "questions": [
    "Question 1",
    "Question 2",
    ...
  ]
}

Where:
- documents: A valid blob URL pointing to a PDF, DOCX, or email file (.eml or .msg).
- questions: A list of user queries in natural language.

üéØ OUTPUT FORMAT
---------------
Return a JSON response like this:

{
  "answers": [
    "Answer 1...",
    "Answer 2...",
    ...
  ]
}

Each answer must:
- Be accurate
- Be based strictly on the content of the document
- Include logic from matching clauses (internally used or externally explainable)

üèóÔ∏è SYSTEM ARCHITECTURE
-----------------------

1. üì• Document Loader
   - Download the file from the provided blob URL.
   - Detect file type: .pdf, .docx, .eml, .msg.
   - Use appropriate parser:
     ‚Ä¢ PDF ‚Üí PyMuPDF or pdfplumber
     ‚Ä¢ DOCX ‚Üí python-docx
     ‚Ä¢ Email ‚Üí extract-msg or Python‚Äôs email module
   - Extract clean, readable text.
   - Convert raw text into structured, semantically meaningful chunks (e.g., by section, heading, paragraph).

2. üîç Chunking & Embedding
   - Split document into intelligent chunks:
     ‚Ä¢ Prefer splitting at headings, subheadings, or logical paragraph boundaries.
     ‚Ä¢ Limit chunk size to ~512 tokens (~300‚Äì500 words).
   - Generate embeddings for each chunk using:
     ‚Ä¢ OpenAI's text-embedding-3-small OR
     ‚Ä¢ Sentence-BERT (e.g., all-MiniLM-L6-v2) via SentenceTransformers
   - Store embeddings in a vector database:
     ‚Ä¢ Primary: FAISS (for lightweight, local use)
     ‚Ä¢ Optional: Pinecone (if cloud persistence needed)
   - Assign metadata to each chunk:
     ‚Ä¢ chunk_id, page_number (if applicable), heading, source_file, etc.

3. ‚ùì Query Embedding & Retrieval
   - For each question in the input list:
     ‚Ä¢ Embed the query using the same model as used for document chunks.
     ‚Ä¢ Perform semantic search in the vector store.
     ‚Ä¢ Retrieve top-k (e.g., k=3) most similar chunks using cosine similarity.
     ‚Ä¢ Return matched chunks with:
       - Text content
       - Metadata (e.g., clause ID, page, heading)
       - Similarity score
   - Log retrieval results for traceability.

4. üß† Answer Generation (LLM Inference)
   - Use a powerful LLM (e.g., GPT-4) to generate answers.
   - Construct prompt with:
     ‚Ä¢ Role: "You are a document analysis assistant."
     ‚Ä¢ Instruction: "Answer based only on the provided clause(s)."
     ‚Ä¢ Dynamic fields:
       - {user_question}
       - {top_k_clauses} (concatenated relevant clause texts)
   - Prompt Template:

     You are a document analysis assistant. Answer based on the given clause(s) only.

     Question: {user_question}

     Relevant Clause(s):
     {top_k_clauses}

     Answer clearly and concisely.

   - Constraints:
     ‚Ä¢ Do not hallucinate.
     ‚Ä¢ If no relevant clause found, respond: "Information not found in the document."
     ‚Ä¢ Keep answers concise and structured.

5. üì§ JSON Response Generator
   - Map each question to its generated answer.
   - Ensure one answer per question in the output array.
   - Internally log:
     ‚Ä¢ Input query
     ‚Ä¢ Retrieved clauses
     ‚Ä¢ LLM input/output
     ‚Ä¢ Final answer
   - Return final response in strict JSON format.

üß™ TESTING & SAMPLE DATA
------------------------
Sample input:

{
  "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?...",
  "questions": [
    "What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?",
    "What is the waiting period for pre-existing diseases (PED) to be covered?",
    "Does this policy cover maternity expenses, and what are the conditions?",
    "What is the waiting period for cataract surgery?",
    "Are the medical expenses for an organ donor covered under this policy?",
    "What is the No Claim Discount (NCD) offered in this policy?",
    "Is there a benefit for preventive health check-ups?",
    "How does the policy define a 'Hospital'?",
    "What is the extent of coverage for AYUSH treatments?",
    "Are there any sub-limits on room rent and ICU charges for Plan A?"
  ]
}

Verify responses against expected answers in the HackRx problem statement.

‚úÖ FUNCTIONAL REQUIREMENTS
--------------------------
| Feature                   | Required? | Notes                                      |
|---------------------------|-----------|--------------------------------------------|
| PDF parsing               | ‚úÖ        | Use PyMuPDF or pdfplumber                  |
| DOCX parsing              | ‚úÖ        | Use python-docx                            |
| Email parsing             | ‚úÖ        | Use extract-msg or email module            |
| Semantic chunking         | ‚úÖ        | Heading-wise or paragraph-wise             |
| Embedding-based retrieval | ‚úÖ        | Use FAISS or Pinecone                      |
| Natural language understanding | ‚úÖ  | Via GPT-4 or equivalent LLM                |
| Explainable output        | ‚úÖ        | Track clause sources and scores            |
| JSON formatted answers    | ‚úÖ        | One answer per query                       |

‚ö° NON-FUNCTIONAL REQUIREMENTS
------------------------------
- Token Efficiency: Use only top-k retrieved chunks (e.g., k=3) in LLM context.
- Low Latency: Aim for <5 seconds per query. Use async processing where possible.
- Reusability: Each module in a separate Python file/class.
- Explainability: Log clause matches, similarity scores, and full LLM prompts/responses.
- Security: Validate and sanitize URLs; avoid arbitrary code execution.
- Modularity: Components must be swappable (e.g., change LLM or vector DB easily).

üß† RECOMMENDED STACK
--------------------
| Component             | Technology                              |
|-----------------------|------------------------------------------|
| Backend API           | FastAPI                                  |
| LLM                   | OpenAI GPT-4                             |
| Embedding Model       | text-embedding-3-small or all-MiniLM-L6-v2 |
| Vector Store          | FAISS (default), Pinecone (optional)     |
| PDF Parsing           | PyMuPDF or pdfplumber                    |
| DOCX Parsing          | python-docx                              |
| Email Parsing         | extract-msg or email module              |
| Data Format           | JSON                                     |
| Optional DB           | PostgreSQL / SQLite (for logs/scoring)   |

üß† BONUS: ADVANCED FEATURES (Optional)
-------------------------------------
- Support multilingual documents via translation API (e.g., Google Translate)
- Add simple UI to input query and highlight source clause
- Return confidence score with each answer (based on similarity + LLM certainty)
- Persistent document indexing using Pinecone/Weaviate
- Redis caching for repeated queries
- Rate limiting and request validation

üöÄ HOW AI SHOULD INTERPRET THIS PLAN
-----------------------------------
- Create one file/module per component:
  ‚Ä¢ loader.py       ‚Üí Handles downloading and file detection
  ‚Ä¢ parser.py       ‚Üí Extracts and cleans text per file type
  ‚Ä¢ chunker.py      ‚Üí Splits text into semantic chunks
  ‚Ä¢ embedder.py     ‚Üí Generates embeddings and stores in FAISS
  ‚Ä¢ retriever.py    ‚Üí Handles query embedding and similarity search
  ‚Ä¢ llm_answer.py   ‚Üí Calls LLM and formats response
  ‚Ä¢ api.py          ‚Üí FastAPI endpoint handler
  ‚Ä¢ utils.py        ‚Üí Shared helpers (logging, sanitization, etc.)

- Comment every function:
  ‚Ä¢ Input type
  ‚Ä¢ Output type
  ‚Ä¢ Purpose

- Ensure the system runs as a single FastAPI app:
  uvicorn api:app --reload

- Maintain separation between business logic and I/O operations.

- Avoid hardcoding:
  ‚Ä¢ Use environment variables for API keys, URLs, chunk size, k-value, etc.

üìä SCORING SYSTEM NOTES
----------------------
Follow HackRx scoring rules:
- Unknown documents ‚Üí 2.0x weight
- Known documents ‚Üí 0.5x weight
- Each question has individual weight (e.g., 1.0 or 2.0)
- Accuracy + traceability = higher score

Ensure every answer is:
- Traceable (linked to clause)
- Correct
- Efficient (low latency, minimal tokens)

üì¶ DEPLOYMENT INSTRUCTIONS
-------------------------
1. Create `.env` file:
   OPENAI_API_KEY=your_key_here
   CHUNK_SIZE=512
   TOP_K=3
   VECTOR_STORE=faiss  # or pinecone

2. Install dependencies:
   pip install -r requirements.txt

3. Run server:
   uvicorn api:app --reload

4. Test with curl:
   curl -X POST http://localhost:8000/api/v1/hackrx/run \
     -H "Content-Type: application/json" \
     -d '{
       "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?...",
       "questions": [
         "What is the grace period for premium payment?"
       ]
     }'

5. Validate output format and correctness.

üèÅ FINAL CHECKLIST BEFORE SUBMISSION
------------------------------------
‚úÖ All 3 file types supported: PDF, DOCX, email
‚úÖ Logs contain clause match info (text, score, metadata)
‚úÖ Output is explainable, correct, and JSON-formatted
‚úÖ Latency acceptable (< 5 seconds per query)
‚úÖ API tested with Postman/curl
‚úÖ Code is modular, commented, and clean
‚úÖ No hardcoded secrets or paths
‚úÖ Requirements.txt includes all dependencies
‚úÖ .env used for configuration
‚úÖ Error handling for invalid URLs, unsupported formats, empty results